“Densities and Fugitive Vectors” in Advances in Techno-Humanities: Case Studies from Culture, Philosophy and the Arts, edited by K. Mak. 28-43. London: Routledge, 2023.

This is the code used in my essay “Densities and Fugitive Vectors.” Note that the code was written under R version 4.2.2 (“Innocent and Trusting”). If this is not the version you are using, you may need to tweak certain elements of the code to ensure functionality. You are welcome to use or adapt this code for your own project(s).

#import and clean texts
setwd(“…”)
library(quanteda)
library(readtext)
litcorpus <- texts(readtext(“Waiting for the Barbarians.txt”, encoding = “UTF-8”))

a <- tokens(litcorpus, what = “word”, remove_punct = TRUE)
b <- tokens_tolower(a)
c <- tokens_select(b, pattern = stopwords(“en”), selection = “remove”)
d <- as.list(c)
i <- paste(unlist(d), collapse = ” “)
corpus <- corpus(i)

#to lemmatize the text
library(spacyr)
sp <- spacy_parse(i, lemma = TRUE, entity = FALSE, pos = FALSE)
sp$token <- sp$lemma
j <- tokens(sp$token, what = “word”, remove_punct = TRUE)
k <- tokens_select(j, c(“s”), selection = “remove”)
m <- as.list(k)
n <- paste(unlist(m), collapse = ” “)
spacy_finalize()

#code for table 3.1 – frequency of the 15 most common words in Waiting for the Barbarians.
library(quanteda.textstats)
library(quanteda.textplots)
p <- dfm(n)
textstat_frequency(p) [1:15]
dfmat <- dfm(corpus)
textstat_frequency(dfmat) [1:15]

#code for table 3.2 – top TF-IDF terms for Waiting for the Barbarians by (changing) partition size.
r <- tokens(n)
s <- tokens_chunk(r, size = 300) #change value for size as required
dfm_toks <- dfm(s)
dfm_tif <- dfm_tfidf(dfm_toks)
topfeatures(dfm_tif, n=2)
topfeatures(dfm_tif [1,], n=2) #to inspect tf-idf features of each section. Change number in square brackets as required

#code for table 3.3 – nearest neighbour for top-rated TF-IDF terms in Waiting for the Barbarians.
wiki_toks <- tokens(n)
feats <- dfm(wiki_toks, verbose = TRUE) %>%
dfm_trim(min_termfreq = 5) %>%
featnames()

wiki_toks <- tokens_select(wiki_toks, feats, padding = TRUE)
wiki_fcm <- fcm(wiki_toks, context = “window”, count = “weighted”, weights = 1 / (1:5), tri = TRUE)
library(“text2vec”) #create word embedding
glove <- GlobalVectors$new(rank = 50, x_max = 10)
wv_main <- glove$fit_transform(wiki_fcm, n_iter = 15, convergence_tol = 0.01, n_threads = 8)

library(quanteda.textmodels)
library(quanteda.textstats)

#to note the (10) closest terms to ‘barbarian’
barbarian <- word_vectors[“barbarian”, , drop = FALSE] #change term as required
cos_sim <- textstat_simil(x = as.dfm(word_vectors), y = as.dfm(barbarian),
method = “cosine”)
head(sort(cos_sim[, 1], decreasing = TRUE), 10)

#code for table 3.4 – top topics by coherence in Waiting for the Barbarians.
#topic modelling with TextmineR
input.dir <- “C:/…”
files.v <- dir(input.dir, “Waiting for the Barbarians.txt”)
chunk.size <- 400 #change this value as required
makeFlexTextChunks <- function(doc.object, chunk.size=chunk.size, percentage=TRUE){
words.lower <- tolower(paste(doc.object, collapse=” “))
words.lower <- gsub(“[^[:alnum:][:space:]’]”, ” “, words.lower)
words.l <- strsplit(words.lower, “\\s+”)
word.v <- unlist(words.l)
x <- seq_along(word.v)

if(percentage){
max.length <- length(word.v)/chunk.size
chunks.l <- split(word.v, ceiling(x/max.length))
}
else {
chunks.l <- split(word.v, ceiling(x/chunk.size))
#deal with small chunks at the end
if(length(chunks.l[[length(chunks.l)]]) <=
length(chunks.l[[length(chunks.l)]])/2){
chunks.l[[length(chunks.l)-1]] <-
c(chunks.l[[length(chunks.l)-1]],
chunks.l[[length(chunks.l)]])
chunks.l[[length(chunks.l)]] <- NULL
}
}
chunks.l <- lapply(chunks.l, paste, collapse=” “)
chunks.df <- do.call(rbind, chunks.l)
return(chunks.df)
}

topic.m <- NULL
for(i in 1:length(files.v)){
doc.object <- scan(paste(input.dir, files.v[i], sep=”/”), what=”character”, sep=”\n”)
chunk.m <- makeFlexTextChunks(doc.object, chunk.size, percentage=FALSE)
segments.m <- cbind(paste(segment = 1:nrow(chunk.m)), chunk.m)
topic.m <- rbind(topic.m, segments.m)
}

documents <- as.data.frame(topic.m, stringsAsFactors=F)
colnames(documents) <- c(“id”, “text”)
documents$id = as.numeric(documents$id)

library(tidytext)
library(tidyr)
library(dplyr)

data <- documents

text_cleaning_tokens %
tidytext::unnest_tokens(word, text)
text_cleaning_tokens$word <- gsub(‘[[:digit:]]+’, ”, text_cleaning_tokens$word)
text_cleaning_tokens$word <- gsub(‘[[:punct:]]+’, ”, text_cleaning_tokens$word)
text_cleaning_tokens % filter(!(nchar(word) == 1))%>%
anti_join(stop_words)
tokens % filter(!(word==””))
tokens % mutate(ind = row_number())
tokens % group_by(id) %>% mutate(ind = row_number()) %>%
tidyr::spread(key = ind, value = word)
tokens [is.na(tokens)] <- “”
tokens <- tidyr::unite(tokens, text,-id,sep =” ” )
tokens$text <- trimws(tokens$text)

library(textmineR)
dtm <- CreateDtm(tokens$text, doc_names = tokens$id, ngram_window = c(1, 2))
tf <- TermDocFreq(dtm = dtm)
original_tf % select(term, term_freq, doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
vocabulary_dtm 1 & tf$doc_freq < nrow(dtm) / 2 ]
dtm = dtm

#determine value for k
k_list <- seq(100,180, by=10)
model_dir <- paste0(“models_”, digest::digest(colnames(dtm), algo = “sha1”))
model_list <- TmParallelApply(X = k_list, FUN = function(k){
m <- FitLdaModel(dtm = dtm, k = k, iterations = 250, burnin = 180, alpha = 0.1, beta = colSums(dtm) / sum(dtm) * 100, optimize_alpha = TRUE, calc_likelihood = FALSE, calc_coherence = TRUE, calc_r2 = FALSE, cpus = 1)
m$k <- k
m
}, export= ls(),
cpus = 2)
coherence_mat <- data.frame(k = sapply(model_list, function(x) nrow(x$phi)), coherence = sapply(model_list, function(x) mean(x$coherence)), stringsAsFactors = FALSE)
coherence_mat
#to plot
plot(coherence_mat, type = “o”, xaxt=”n”)
axis(1, at = seq(100, 180, by = 10), las=2)

#build model with best k value
set.seed(12345)
model <- FitLdaModel(dtm = dtm, k = 130, iterations = 500, burnin = 180, alpha = 0.1, beta = 0.05, optimize_alpha = TRUE, calc_likelihood = TRUE, calc_coherence = TRUE, calc_r2 = TRUE, cpus = 2)

#create data frame to plot (here we plot the coherence string)
cm <- data.frame(model$coherence)

#create x axis vector length (ie. the number of topics modelled: the k value)
xlen <- 1:130

library(ggplot2)
ggplot(cm, aes(x = xlen, y = model.coherence)) +
geom_point() +
geom_line(group = 1)+
ggtitle(“Best Topic by Coherence Score”) + theme_minimal() +
scale_x_continuous(breaks = seq(1,130,1)) + ylab(“Coherence”)

#to get the top terms of each topic (reports 10 terms for each topic)
model$top_terms <- GetTopTerms(phi = model$phi, M = 10)
model$top_terms
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100

#native topic labelling
model$labels 0.15, dtm = dtm, M = 1)
model$summary <- data.frame(topic = rownames(model$phi), label = model$labels, coherence = round(model$coherence, 3), prevalence = round(model$prevalence,3), top_terms = apply(model$top_terms, 2, function(x){ paste(x, collapse = “, “)}),stringsAsFactors = FALSE)

model$summary[ order(model$summary$coherence, decreasing = TRUE) , ][ 1:10 , ]

#code for table 3.5 – top topics by prevalence in Waiting for the Barbarians.
model$summary[ order(model$summary$prevalence, decreasing = TRUE) , ][ 1:10 , ]

#code for figure 3.1 – feature co-occurrence network of Waiting for the Barbarians.
fcmat <- fcm(n, context = “window”, count = “frequency”, tri = FALSE)
feat <- names(topfeatures(fcmat, 200))
fcm_select(fcmat, pattern = feat) %>%
textplot_network(min_freq = 0.85, omit_isolated = TRUE, edge_color = “#768aa5”, edge_alpha = 0.5, edge_size = 1, vertex_color = “#536174”, vertex_size = 2, vertex_labelcolor = “#000000”, vertex_labelfont = NULL, vertex_labelsize = 3, offset = NULL)

#code for figure 3.2 – section network of Coetzee’s Waiting for the Barbarians.
mt_b <- dfm_trim(dfm_toks, min_termfreq = 5)
sim_b <- textstat_proxy(mt_b, margin = “documents”, method = “cosine”)
textplot_network(quanteda:::as.fcm(as(sim_b, “dgTMatrix”)), min_freq = 0.89, omit_isolated = FALSE, edge_color = “#768aa5”, edge_alpha = 0.5, edge_size = 1, vertex_color = “#536174”, vertex_size = 2, vertex_labelcolor = “#000000”, vertex_labelfont = NULL, vertex_labelsize = 3, offset = NULL)

#code for figure 3.3 – graph showing the cumulative sum of the emotional valence of Waiting for the Barbarians and the position of sections 53, 99, and 100.
library(syuzhet)
test <- get_text_as_string(“Waiting for the Barbarians.txt”)
sy_vec <- get_sentences(test)
sy_vec_sent <- get_sentiment(sy_vec, method = “syuzhet”)
sy_sent_cs <- cumsum(sy_vec_sent)

#to inspect a range of sentiment scores
sy_sent_cs[2341:2374]
#to inspect associated sentences from the novel
sy_vec[2341:2374]

plot(sy_sent_cs, type=”l”, xlab = “Narrative Time”, ylab= “Emotional Valence”, panel.first = rect(c(2341, 4364, 4434), -1e6, c(2374, 4433, 4497), 1e6, col=’dark grey’))

#code for figure 3.4 – graph showing the cumulative sum of the emotional valence of Waiting for the Barbarians and the change in frequency of negative emotional language that occurs at section 53.
gradient1 <- (sy_sent_cs[2341] – sy_sent_cs[1]) / (2341 – 1)
gradient2 <- (sy_sent_cs[4539] – sy_sent_cs[2374]) / (4539 – 2374)
lines(x = c(1 + 600, 2341 – 250), y = c(sy_sent_cs[1 + 600] + 75, sy_sent_cs[2341 – 250] + 75), col = “black”, lwd = 2, lty = 2)
lines(x = c(2374 + 250, 4539 – 250), y = c(sy_sent_cs[2374 + 250] + 75, sy_sent_cs[4539 – 250] + 75), col = “black”, lwd = 2, lty = 2)

#code for figure 3.5 – graph showing the distribution of topic 119 (warrant_officer) in Waiting for the Barbarians.
plot(model$theta[,119], type = “l”, main=”Theta for Topic 119 (warrant_officer)”, xlab=”section”, ylab=”value”, xlim=c(0, 166), ylim=c(0, 0.55))

#code for figure 3.6 – graph showing the distribution of topic 10 (day_day) in Waiting for the Barbarians.
plot(model$theta[,10], type = “l”, main=”Theta for Topic 10 (day_day)”, xlab=”section”, ylab=”value”, xlim=c(0, 166), ylim=c(0, 0.55))
