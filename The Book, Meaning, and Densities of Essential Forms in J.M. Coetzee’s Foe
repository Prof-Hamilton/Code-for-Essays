# Computational Literary Analysis of J.M. Coetzee's *Foe*

R code for computational text analysis accompanying:

**Hamilton, G. (2024).** The Book, Meaning, and Densities of Essential Forms in J.M. Coetzee's *Foe*. *Lit: Literature Interpretation Theory*, 35(1), 18-38. https://doi.org/10.1080/10436928.2024.2305602

## Overview

This code implements computational methods to analyze Coetzee's *Foe* through Deleuze and Guattari's concept of textual "densities" and "essential forms." 
The analysis reveals latent semantic structures, word clusters, and thematic networks that constitute the novel's objective structured meaning.

## Requirements

**R version:** 4.2.2

**Required packages:**
- Text analysis: `quanteda`, `quanteda.textstats`, `quanteda.textplots`, `quanteda.textmodels`, `readtext`
- NLP: `spacyr` (requires SpaCy installation)
- Embeddings: `text2vec`, `uwot`
- Topic modeling: `textmineR`, `tidytext`, `tidyr`
- Sentiment: `syuzhet`
- Visualization: `ggplot2`, `dplyr`

## Data Requirements

The code expects text file(s) of Coetzee's *Foe* in `.txt` format (UTF-8). The novel text is under copyright and must be obtained legally.

## Usage

Set working directory to folder containing the text file:

```r
setwd("path/to/your/data")
source("Deleuze_and_Foe_Analysis.R")
```

## Key Parameters

Customizable settings at top of script:

```r
CHUNK_SIZE <- 400          # Section size in words
EMBEDDING_DIM <- 50        # Word vector dimensions
TOPIC_MODEL_K <- 40        # Number of topics
TOPIC_MODEL_SEED <- 12345  # Reproducibility seed
```

## Citation

```bibtex
@article{hamilton2024book,
  title={The Book, Meaning, and Densities of Essential Forms in {J.M. Coetzee's} \textit{Foe}},
  author={Hamilton, Grant},
  journal={Lit: Literature Interpretation Theory},
  volume={35},
  number={1},
  pages={18--38},
  year={2024},
  publisher={Taylor \& Francis},
  doi={10.1080/10436928.2024.2305602}
}
```

## License

**Code:** Available for scholarly use under fair use provisions  
**Essay:** Open Access, CC BY-NC-ND 4.0  
**Novel:** J.M. Coetzee's *Foe* (1986) is under copyright

## Contact

For questions or issues, please contact:

Grant Hamilton: hamilton@cuhk.edu.hk
Department of English, Chinese University of Hong Kong


# ============================================================================
# Computational Literary Analysis of J.M. Coetzee's Foe
# ============================================================================
# 
# Citation:
# Hamilton, G. (2024). The Book, Meaning, and Densities of Essential Forms in 
# J.M. Coetzee's Foe. Lit: Literature Interpretation Theory, 35(1), 18-38.
# DOI: 10.1080/10436928.2024.2305602
#
# Code implements Deleuze and Guattari's concept of textual "densities" and
# "essential forms" through computational text analysis methods
#
# R version: 4.2.2
# Last updated: March 2024
# 
# License: Code available for scholarly use under fair use provisions
# ============================================================================

# ============================================================================
# SETUP AND CONFIGURATION
# ============================================================================

# Set working directory to folder containing Foe text file(s)
# setwd("...")  # User should set their own path

# Configuration parameters
CHUNK_SIZE <- 400          # Words per section for analysis
EMBEDDING_DIM <- 50        # Dimensionality of word vectors
GLOVE_ITERATIONS <- 15     # GloVe training iterations
TOPIC_MODEL_K <- 40        # Number of topics for LDA
TOPIC_MODEL_SEED <- 12345  # Seed for reproducibility

# ============================================================================
# SECTION 1: TEXT LOADING AND PREPROCESSING
# ============================================================================

# Load required packages
library(quanteda)
library(readtext)
library(spacyr)
library(quanteda.textstats)
library(quanteda.textplots)

# Load text file(s) - expects UTF-8 encoded .txt files of Coetzee's Foe
litcorpus <- texts(readtext("*.txt", encoding = "UTF-8"))

# Tokenization and cleaning
tokens_raw <- tokens(litcorpus, what = "word", remove_punct = TRUE)
tokens_lower <- tokens_tolower(tokens_raw)
tokens_clean <- tokens_select(tokens_lower, pattern = stopwords("en"), selection = "remove")

# Convert to single document string
tokens_list <- as.list(tokens_clean)
text_combined <- paste(unlist(tokens_list), collapse = " ")

# Create corpus object
corpus <- corpus(text_combined)

# ============================================================================
# SECTION 2: LEMMATIZATION
# ============================================================================

# Note: Requires SpaCy installation (run spacy_install() first time)
# Lemmatize text using SpaCy
sp <- spacy_parse(text_combined, lemma = TRUE, entity = FALSE, pos = FALSE)
sp$token <- sp$lemma

# Process lemmatized tokens
tokens_lemma <- tokens(sp$token, what = "word", remove_punct = TRUE)
tokens_lemma <- tokens_select(tokens_lemma, c("s"), selection = "remove")
lemma_list <- as.list(tokens_lemma)
text_lemmatized <- paste(unlist(lemma_list), collapse = " ")

# Clean up SpaCy
spacy_finalize()

# ============================================================================
# SECTION 3: WORD FREQUENCY ANALYSIS
# ============================================================================

# Calculate word frequencies (Table 1 in essay)
dfm_lemma <- dfm(text_lemmatized)
freq_table <- textstat_frequency(dfm_lemma)[1:10,]
print("Top 10 most frequent words (lemmatized):")
print(freq_table)

# Compare with non-lemmatized version
dfmat_orig <- dfm(corpus)
freq_table_orig <- textstat_frequency(dfmat_orig)[1:10,]

# ============================================================================
# SECTION 4: FEATURE CO-OCCURRENCE NETWORK
# ============================================================================

# Create feature co-occurrence matrix
fcmat <- fcm(text_lemmatized, context = "window", count = "frequency", tri = FALSE)
feat <- names(topfeatures(fcmat, 200))

# Figure 1: Feature network (top 200 words, threshold 0.85)
fcm_select(fcmat, pattern = feat) %>%
  textplot_network(min_freq = 0.85, omit_isolated = TRUE, 
                   edge_color = "#768aa5", edge_alpha = 0.5, edge_size = 1, 
                   vertex_color = "#536174", vertex_size = 2, 
                   vertex_labelcolor = "#000000", vertex_labelfont = NULL, 
                   vertex_labelsize = 3, offset = NULL)

# Higher definition network centered on Friday (threshold 0.995)
fcm_select(fcmat, pattern = feat) %>%
  textplot_network(min_freq = 0.995, omit_isolated = TRUE, 
                   edge_color = "#768aa5", edge_alpha = 0.5, edge_size = 1, 
                   vertex_color = "#536174", vertex_size = 2, 
                   vertex_labelcolor = "#000000", vertex_labelfont = NULL, 
                   vertex_labelsize = 3, offset = NULL)

# ============================================================================
# SECTION 5: TF-IDF ANALYSIS
# ============================================================================

# Partition text and calculate TF-IDF weights (Table 2 in essay)
tokens_chunked <- tokens(text_lemmatized)
tokens_sections <- tokens_chunk(tokens_chunked, size = CHUNK_SIZE)
dfm_toks <- dfm(tokens_sections)
dfm_tfidf <- dfm_tfidf(dfm_toks)

# Top weighted features across all sections
top_features_global <- topfeatures(dfm_tfidf, n = 10)
print("Top 10 TF-IDF weighted features:")
print(top_features_global)

# Example: Top features for section 1 (change index as needed)
top_features_section1 <- topfeatures(dfm_tfidf[1,], n = 10)

# ============================================================================
# SECTION 6: WORD EMBEDDINGS (WORD2VEC/GLOVE)
# ============================================================================

library(text2vec)
library(quanteda.textmodels)
library(uwot)
library(dplyr)
library(ggplot2)

# Train GloVe word embeddings
glove <- GlobalVectors$new(rank = EMBEDDING_DIM, x_max = 10)
wv_main <- glove$fit_transform(fcmat, n_iter = GLOVE_ITERATIONS,
                                convergence_tol = 0.01, n_threads = 8)
wv_context <- glove$components
word_vectors <- wv_main + t(wv_context)

# Find nearest neighbors to "tongue" (Table 3 in essay)
tongue_vec <- word_vectors["tongue", , drop = FALSE]
cos_sim_tongue <- textstat_simil(x = as.dfm(word_vectors), y = as.dfm(tongue_vec),
                                  method = "cosine")
nearest_tongue <- head(sort(cos_sim_tongue[, 1], decreasing = TRUE), 10)
print("10 nearest words to 'tongue':")
print(nearest_tongue)

# ============================================================================
# SECTION 7: WORD EMBEDDING VISUALIZATION
# ============================================================================

# Reduce to 2D using UMAP for visualization (Figure 2 in essay)
word2vec_umap <- umap(word_vectors, n_components = 2, metric = "cosine", 
                      n_neighbors = 100, min_dist = 0, spread = 1)

# Convert to dataframe
df_word2vec_umap <- as.data.frame(word2vec_umap, stringsAsFactors = FALSE)
df_word2vec_umap$word <- rownames(word_vectors)
colnames(df_word2vec_umap) <- c("UMAP1", "UMAP2", "word")
df_word2vec_umap$technique <- 'Word2Vec'

# Select 40 words around "foe"
token_f <- "foe"
embedding_vector_f <- t(matrix(word_vectors[token_f,])) 
cos_sim_f <- sim2(x = word_vectors, y = embedding_vector_f, 
                  method = "cosine", norm = "l2")
select_f <- data.frame(word = rownames(as.data.frame(
  head(sort(cos_sim_f[,1], decreasing = TRUE), 40))))
select_words_f <- df_word2vec_umap %>% inner_join(y = select_f, by = "word")

# Select 40 words around "write"
token_w <- "write"
embedding_vector_w <- t(matrix(word_vectors[token_w,])) 
cos_sim_w <- sim2(x = word_vectors, y = embedding_vector_w, 
                  method = "cosine", norm = "l2")
select_w <- data.frame(word = rownames(as.data.frame(
  head(sort(cos_sim_w[,1], decreasing = TRUE), 40))))
select_words_w <- df_word2vec_umap %>% inner_join(y = select_w, by = "word")

# Plot: Figure 2 - Word embeddings with "foe" (red) and "write" (blue)
foe_and_write_emb <- ggplot(df_word2vec_umap) +
  geom_point(aes(x = UMAP1, y = UMAP2), colour = 'dark gray', 
             size = 3.5, alpha = 1/3) +
  theme_void() +
  geom_point(data = select_words_f, aes(x = UMAP1, y = UMAP2), 
             color = "red", size = 3.5) +
  geom_point(data = select_words_w, aes(x = UMAP1, y = UMAP2), 
             color = "blue", size = 3.5)
print(foe_and_write_emb)

# ============================================================================
# SECTION 8: SECTION NETWORK ANALYSIS
# ============================================================================

# Figure 3: Visualize semantic relationships between sections
mt_trimmed <- dfm_trim(dfm_toks, min_termfreq = 5)
sim_matrix <- textstat_proxy(mt_trimmed, margin = "documents", method = "cosine")
textplot_network(quanteda:::as.fcm(as(sim_matrix, "dgTMatrix")), 
                 min_freq = 0.97, omit_isolated = TRUE, 
                 edge_color = "#768aa5", edge_alpha = 0.5, edge_size = 1, 
                 vertex_color = "#536174", vertex_size = 2, 
                 vertex_labelcolor = "#000000", vertex_labelfont = NULL, 
                 vertex_labelsize = 3, offset = NULL)

# ============================================================================
# SECTION 9: TOPIC MODELING PREPARATION
# ============================================================================

library(tidytext)
library(tidyr)
library(textmineR)

# Function to chunk text into sections
makeFlexTextChunks <- function(doc.object, chunk.size = 400, percentage = TRUE) {
  words.lower <- tolower(paste(doc.object, collapse = " "))
  words.lower <- gsub("[^[:alnum:][:space:]']", " ", words.lower)
  words.l <- strsplit(words.lower, "\\s+")
  word.v <- unlist(words.l)
  x <- seq_along(word.v)
  
  if (percentage) {
    max.length <- length(word.v) / chunk.size
    chunks.l <- split(word.v, ceiling(x / max.length))
  } else {
    chunks.l <- split(word.v, ceiling(x / chunk.size))
    # Handle small final chunks
    if (length(chunks.l[[length(chunks.l)]]) <= 
        length(chunks.l[[length(chunks.l)]]) / 2) {
      chunks.l[[length(chunks.l) - 1]] <- 
        c(chunks.l[[length(chunks.l) - 1]], chunks.l[[length(chunks.l)]])
      chunks.l[[length(chunks.l)]] <- NULL
    }
  }
  chunks.l <- lapply(chunks.l, paste, collapse = " ")
  chunks.df <- do.call(rbind, chunks.l)
  return(chunks.df)
}

# Load and chunk text files
# input.dir <- "..."  # Set to your directory
# files.v <- dir(input.dir, "*.txt")
# 
# topic.m <- NULL
# for (i in 1:length(files.v)) {
#   doc.object <- scan(paste(input.dir, files.v[i], sep = "/"), 
#                      what = "character", sep = "\n")
#   chunk.m <- makeFlexTextChunks(doc.object, CHUNK_SIZE, percentage = FALSE)
#   segments.m <- cbind(paste(segment = 1:nrow(chunk.m)), chunk.m)
#   topic.m <- rbind(topic.m, segments.m)
# }

# [Code available when text files are loaded]
# Uncomment above section and set correct paths to run

# Process documents for topic modeling
# documents <- as.data.frame(topic.m, stringsAsFactors = FALSE)
# colnames(documents) <- c("id", "text")
# documents$id <- as.numeric(documents$id)
# 
# data <- documents

# Clean and tokenize for topic modeling
# text_cleaning_tokens <- data %>% 
#   tidytext::unnest_tokens(word, text)
# text_cleaning_tokens$word <- gsub('[[:digit:]]+', '', text_cleaning_tokens$word)
# text_cleaning_tokens$word <- gsub('[[:punct:]]+', '', text_cleaning_tokens$word)
# text_cleaning_tokens <- text_cleaning_tokens %>% 
#   filter(!(nchar(word) == 1)) %>% 
#   anti_join(stop_words)
# 
# tokens <- text_cleaning_tokens %>% filter(!(word == ""))
# tokens <- tokens %>% mutate(ind = row_number())
# tokens <- tokens %>% group_by(id) %>% mutate(ind = row_number()) %>%
#   tidyr::spread(key = ind, value = word)
# tokens[is.na(tokens)] <- ""
# tokens <- tidyr::unite(tokens, text, -id, sep = " ")
# tokens$text <- trimws(tokens$text)

# ============================================================================
# SECTION 10: AUTOMATIC K SELECTION FOR TOPIC MODELING
# ============================================================================

# Create Document-Term Matrix
# dtm <- CreateDtm(tokens$text, doc_names = tokens$id, ngram_window = c(1, 2))
# tf <- TermDocFreq(dtm = dtm)
# original_tf <- tf %>% select(term, term_freq, doc_freq)
# rownames(original_tf) <- 1:nrow(original_tf)
# vocabulary_dtm <- tf$term[tf$term_freq > 1 & tf$doc_freq < nrow(dtm) / 2]

# Test different K values (10, 20, 30, ..., 80)
# k_list <- seq(10, 80, by = 10)
# model_dir <- paste0("models_", digest::digest(colnames(dtm), algo = "sha1"))
# 
# model_list <- TmParallelApply(X = k_list, FUN = function(k) {
#   m <- FitLdaModel(dtm = dtm, k = k, iterations = 250, burnin = 180, 
#                    alpha = 0.1, beta = colSums(dtm) / sum(dtm) * 100, 
#                    optimize_alpha = TRUE, calc_likelihood = FALSE, 
#                    calc_coherence = TRUE, calc_r2 = FALSE, cpus = 1)
#   m$k <- k
#   m
# }, export = ls(), cpus = 2)
# 
# coherence_mat <- data.frame(
#   k = sapply(model_list, function(x) nrow(x$phi)), 
#   coherence = sapply(model_list, function(x) mean(x$coherence)), 
#   stringsAsFactors = FALSE
# )
# print(coherence_mat)

# Plot coherence scores by K
# plot(coherence_mat, type = "o", xaxt = "n", 
#      main = "Optimal K Selection", xlab = "K", ylab = "Coherence")
# axis(1, at = seq(10, 80, by = 10), las = 2)

# ============================================================================
# SECTION 11: FINAL TOPIC MODEL (K=40)
# ============================================================================

# Fit final LDA model with K=40 (Tables 6-7 in essay)
# set.seed(TOPIC_MODEL_SEED)
# model <- FitLdaModel(dtm = dtm, k = TOPIC_MODEL_K, iterations = 500, 
#                      burnin = 180, alpha = 0.1, beta = 0.05, 
#                      optimize_alpha = TRUE, calc_likelihood = TRUE, 
#                      calc_coherence = TRUE, calc_r2 = TRUE, cpus = 2)

# Plot topic coherence
# cm <- data.frame(model$coherence)
# xlen <- 1:TOPIC_MODEL_K
# 
# ggplot(cm, aes(x = xlen, y = model.coherence)) +
#   geom_point() +
#   geom_line(group = 1) +
#   ggtitle("Best Topic by Coherence Score") + 
#   theme_minimal() +
#   scale_x_continuous(breaks = seq(1, TOPIC_MODEL_K, 1)) + 
#   ylab("Coherence")

# Extract top terms for each topic
# model$top_terms <- GetTopTerms(phi = model$phi, M = 10)
# print("Top 10 terms per topic:")
# print(model$top_terms)

# Calculate topic prevalence
# model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
# print("Topic prevalence:")
# print(model$prevalence)

# Plot prevalence vs alpha
# plot(model$prevalence, model$alpha, xlab = "Prevalence", ylab = "Alpha",
#      main = "Topic Prevalence vs Alpha")

# Native topic labeling
# model$labels <- LabelTopics(assignments = model$theta > 0.15, dtm = dtm, M = 1)
# print("Topic labels:")
# print(model$labels)

# Create summary table
# model$summary <- data.frame(
#   topic = rownames(model$phi), 
#   label = model$labels, 
#   coherence = round(model$coherence, 3), 
#   prevalence = round(model$prevalence, 3), 
#   top_terms = apply(model$top_terms, 2, function(x) { 
#     paste(x, collapse = ", ") 
#   }),
#   stringsAsFactors = FALSE
# )

# Top 10 topics by coherence
# print("Top 10 topics by coherence:")
# print(model$summary[order(model$summary$coherence, decreasing = TRUE), ][1:10, ])

# Top 10 topics by prevalence
# print("Top 10 topics by prevalence:")
# print(model$summary[order(model$summary$prevalence, decreasing = TRUE), ][1:10, ])

# ============================================================================
# SECTION 12: TOPIC DISTRIBUTION VISUALIZATION
# ============================================================================

# Plot theta for Topic 24 across novel (Figure 5 in essay)
# plot(model$theta[, 24], type = "l", main = "Theta for Topic 24 across Foe",
#      xlab = "Section", ylab = "Theta Value",
#      xlim = c(0, 114), ylim = c(0, 0.8))

# View theta and text for specific topic/section
# print("Theta for Topic 24:")
# print(model$theta[, 24])
# print("Text of section 24:")
# print(data$text[24])

# ============================================================================
# SECTION 13: SENTIMENT ANALYSIS
# ============================================================================

library(syuzhet)

# Load text file for sentiment analysis (Figure 6 in essay)
# text <- get_text_as_string("Coetzee_Foe.txt")  # Set correct path

# Extract sentences
# sy_vec <- get_sentences(text)

# Calculate sentiment scores
# sy_vec_sent <- get_sentiment(sy_vec, method = "syuzhet")

# Plot sentiment changes across narrative
# plot(sy_vec_sent, type = "h", 
#      main = "Changes in Sentiment in Foe", 
#      xlab = "Narrative Time", 
#      ylab = "Emotional Valence")

# Syuzhet smoothed plot
# simple_plot(sy_vec_sent)

# Cumulative sentiment trajectory
# foe_sent_cs <- cumsum(sy_vec_sent)
# plot(foe_sent_cs, type = "l", 
#      main = "Cumulative Sentiment in Foe", 
#      xlab = "Narrative Time", 
#      ylab = "Emotional Valence")

# Examine specific sentiment scores
# print("Sentiment scores for sentences 1875-1880:")
# print(foe_sent_cs[1875:1880])

# View corresponding text
# print("Sentences 1875-1880:")
# print(sy_vec[1875:1880])

# ============================================================================
# END OF ANALYSIS
# ============================================================================
